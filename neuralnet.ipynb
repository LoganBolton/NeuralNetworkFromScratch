{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73da0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11d4c083-0801-48a8-ba8c-0f57ed4a1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ubyte_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the magic number\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        \n",
    "        if magic_number == 2051:  # Magic number for images\n",
    "            num_images = int.from_bytes(f.read(4), 'big')\n",
    "            rows = int.from_bytes(f.read(4), 'big')\n",
    "            cols = int.from_bytes(f.read(4), 'big')\n",
    "            images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "            return images\n",
    "        \n",
    "        elif magic_number == 2049:  # Magic number for labels\n",
    "            num_labels = int.from_bytes(f.read(4), 'big')\n",
    "            labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            return labels\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Invalid magic number in file header')\n",
    "\n",
    "train_images_path = './data/train-images-idx3-ubyte'\n",
    "train_labels_path = './data/train-labels-idx1-ubyte'\n",
    "\n",
    "train_images = read_ubyte_file(train_images_path)\n",
    "train_labels = read_ubyte_file(train_labels_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4909754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fb97853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "# go from (60000, 28, 28) to (60000, 784)\n",
    "X = train_images.reshape(train_images.shape[0],train_images.shape[1]*train_images.shape[2])\n",
    "print(train_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "787dfe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    # each pixel from image needs to point to 10 hidden neurons\n",
    "    W1 = np.random.randn(784, 10) * 0.01 \n",
    "    #1 bias for each of the 10 hidden neurons\n",
    "    b1 = np.random.randn(1, 10) * 0.01   \n",
    "    \n",
    "    # each of the 10 hidden neurons needs to point to each of the 10 output neurons\n",
    "    W2 = np.random.randn(10, 10) * 0.01  \n",
    "    # 1 bias for each of the 10 output neurons\n",
    "    b2 = np.random.randn(1, 10) * 0.01   \n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81f61cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c0c1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afe0c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    # Initialize a matrix of zeros with shape (number of labels, number of classes)\n",
    "    one_hot = np.zeros((labels.size, num_classes))\n",
    "    \n",
    "    # Set the corresponding index to 1 for each label\n",
    "    one_hot[np.arange(labels.size), labels] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "Y = one_hot_encode(train_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f91b00ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A2, Y):\n",
    "    m = Y.shape[0]\n",
    "    Y_int = np.argmax(Y, axis=1)\n",
    "\n",
    "    log_probs = -np.log(A2[np.arange(m), Y_int])\n",
    "    loss = np.sum(log_probs) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94150ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2, Y):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = soft_max(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa76a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(X, Y, Z1, A1, Z2, A2, W1, W2, b1, b2, learning_rate):\n",
    "    # used to find the normalized gradient\n",
    "    # m = num of training examples in batch\n",
    "    m = X.shape[0]\n",
    "    dZ2 = A2 - Y\n",
    "    \n",
    "    # multiply the output of h1 by the change in the output\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    \n",
    "    dA = np.dot(dZ2, W2.T)\n",
    "    \n",
    "    # relu derivative\n",
    "    dZ1 = dA * (Z1 > 0)\n",
    "    \n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    \n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24a2a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a173f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, W1, b1, W2, b2, learning_rate, epochs):\n",
    "    for i in range(epochs):\n",
    "        # forward\n",
    "        Z1, A1, Z2, A2 = forward(X, W1, b1, W2, b2, Y)\n",
    "        loss = compute_loss(A2, Y)\n",
    "        #backward\n",
    "        dW1, db1, dW2, db2 = back_prop(X, Y, Z1, A1, Z2, A2, W1, W2, b1, b2, learning_rate)\n",
    "        \n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch {i}: loss = {loss}')\n",
    "    return W1, b1, W2, b2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "217b2fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 2.3807194896388113\n",
      "Epoch 100: loss = 2.3025121226150183\n",
      "Epoch 200: loss = 2.3022641159634705\n",
      "Epoch 300: loss = 2.3020613366651577\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = init_params()\n",
    "\n",
    "\n",
    "W1, b1, W2, b2 = train(X, Y, W1, b1, W2, b2, 0.01, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
