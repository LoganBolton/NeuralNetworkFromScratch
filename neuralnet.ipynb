{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73da0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "11d4c083-0801-48a8-ba8c-0f57ed4a1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ubyte_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the magic number\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        \n",
    "        if magic_number == 2051:  # Magic number for images\n",
    "            num_images = int.from_bytes(f.read(4), 'big')\n",
    "            rows = int.from_bytes(f.read(4), 'big')\n",
    "            cols = int.from_bytes(f.read(4), 'big')\n",
    "            images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "            return images\n",
    "        \n",
    "        elif magic_number == 2049:  # Magic number for labels\n",
    "            num_labels = int.from_bytes(f.read(4), 'big')\n",
    "            labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            return labels\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Invalid magic number in file header')\n",
    "\n",
    "train_images_path = './data/train-images-idx3-ubyte'\n",
    "train_labels_path = './data/train-labels-idx1-ubyte'\n",
    "\n",
    "train_images = read_ubyte_file(train_images_path)\n",
    "train_labels = read_ubyte_file(train_labels_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c4909754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8fb97853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "# go from (60000, 28, 28) to (60000, 784)\n",
    "X = train_images.reshape(train_images.shape[0],train_images.shape[1]*train_images.shape[2])\n",
    "print(train_images[0])\n",
    "X = X/255.0\n",
    "print(X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f6a7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "787dfe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    # each pixel from image needs to point to 10 hidden neurons\n",
    "    W1 = np.random.randn(784, 10) * 0.01 \n",
    "    #1 bias for each of the 10 hidden neurons\n",
    "    b1 = np.random.randn(1, 10) * 0.01   \n",
    "    \n",
    "    # each of the 10 hidden neurons needs to point to each of the 10 output neurons\n",
    "    W2 = np.random.randn(10, 10) * 0.01  \n",
    "    # 1 bias for each of the 10 output neurons\n",
    "    b2 = np.random.randn(1, 10) * 0.01   \n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "81f61cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0c0c1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "afe0c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    # Initialize a matrix of zeros with shape (number of labels, number of classes)\n",
    "    one_hot = np.zeros((labels.size, num_classes))\n",
    "    \n",
    "    # Set the corresponding index to 1 for each label\n",
    "    one_hot[np.arange(labels.size), labels] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "Y = one_hot_encode(train_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f91b00ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A2, Y):\n",
    "    m = Y.shape[0]\n",
    "    Y_int = np.argmax(Y, axis=1)\n",
    "\n",
    "    log_probs = -np.log(A2[np.arange(m), Y_int])\n",
    "    loss = np.sum(log_probs) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "94150ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2, Y):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = soft_max(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aa76a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(X, Y, Z1, A1, Z2, A2, W1, W2, b1, b2, learning_rate):\n",
    "    # used to find the normalized gradient\n",
    "    # m = num of training examples in batch\n",
    "    m = X.shape[0]\n",
    "    dZ2 = A2 - Y\n",
    "    \n",
    "    # multiply the output of h1 by the change in the output\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    \n",
    "    dA = np.dot(dZ2, W2.T)\n",
    "    \n",
    "    # relu derivative\n",
    "    dZ1 = dA * (Z1 > 0)\n",
    "    \n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    \n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "24a2a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a173f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, W1, b1, W2, b2, learning_rate, epochs):\n",
    "    for i in range(epochs):\n",
    "        # forward\n",
    "        Z1, A1, Z2, A2 = forward(X, W1, b1, W2, b2, Y)\n",
    "        loss = compute_loss(A2, Y)\n",
    "        #backward\n",
    "        dW1, db1, dW2, db2 = back_prop(X, Y, Z1, A1, Z2, A2, W1, W2, b1, b2, learning_rate)\n",
    "        \n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'Epoch {i}: loss = {loss}')\n",
    "    return W1, b1, W2, b2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "217b2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cf89790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.3364361134251093\n",
      "Epoch 50: loss = 0.3314118984759991\n",
      "Epoch 100: loss = 0.3267835500852011\n",
      "Epoch 150: loss = 0.32248727134860666\n",
      "Epoch 200: loss = 0.3185100010750274\n",
      "Epoch 250: loss = 0.31480088803004047\n",
      "Epoch 300: loss = 0.3113672855291566\n",
      "Epoch 350: loss = 0.3081932827409766\n",
      "Epoch 400: loss = 0.30525795666774447\n",
      "Epoch 450: loss = 0.30253662615529436\n",
      "Epoch 500: loss = 0.2999747429167423\n",
      "Epoch 550: loss = 0.29757754976999934\n",
      "Epoch 600: loss = 0.2953212435538508\n",
      "Epoch 650: loss = 0.29317466295991984\n",
      "Epoch 700: loss = 0.29114447796510184\n",
      "Epoch 750: loss = 0.28921723492961104\n",
      "Epoch 800: loss = 0.2873862807891883\n",
      "Epoch 850: loss = 0.2856283276561296\n",
      "Epoch 900: loss = 0.2839453501250728\n",
      "Epoch 950: loss = 0.28233641333972836\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = train(X, Y, W1, b1, W2, b2, 0.1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6ac9cc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92137\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, axis=1)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    # Convert one-hot encoded Y to integer labels\n",
    "    Y_int = np.argmax(Y, axis=1)\n",
    "    return np.sum(predictions == Y_int) / Y_int.size\n",
    "\n",
    "#returns A2\n",
    "_, _, _, A2 = forward(X, W1, b1, W2, b2, Y)\n",
    "\n",
    "# Get predictions from the output layer activations\n",
    "predictions = get_predictions(A2)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = get_accuracy(predictions, Y)\n",
    "print(f'Accuracy: {accuracy:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
